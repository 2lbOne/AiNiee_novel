# SakuraLLM 性能优化指南

## 0. 前言
SakuraLLM 默认设置比较保守，性能较差。

通过本文的优化，您可以获得多至5倍的性能提升。

本文档假定您已经使用 SakuraLLM 官方提供的一键包进行部署，并且基本翻译流程已经调通。

如果您使用其他部署方式或需要基础部署教程，请自行调整或寻找相应文档。

## 1. 选择合适的模型
根据显存大小选择适合的模型并放入SakuraLLM文件夹。

7B 模型的翻译能力较差，除非显存受限，否则应避免使用。

32B 模型与 14B 模型相比，速度显著降低，但是翻译能力的提升有限，除 4090 用户以外不推荐使用。

| 显存大小 | 推荐模型大小 | 推荐模型及链接                                           |
|----------|--------------|-----------------------------------------------------------|
| 8G       | 7B           | [GalTransl-7B-v1-IQ4_XS.gguf](https://huggingface.co/SakuraLLM/GalTransl-v1/blob/main/GalTransl-7B-v1-IQ4_XS.gguf) |
| 11G/12G/16G/24G | 14B        | [Sakura-14B-Qwen2beta-v0.9.2-GGUF](https://huggingface.co/SakuraLLM/Sakura-14B-Qwen2beta-v0.9.2-GGUF/blob/main/sakura-14b-qwen2beta-v0.9.2-iq4xs.gguf) |

## 2. 启用 SakuraLLM 多实例支持
从 [项目主页Script目录](/SakuraLLMScript/) 下载 `common.bat` 以及与您的显存大小对应的启动脚本，并将其放入 SakuraLLM 文件夹。

| 显存大小 | 启动脚本名称            |
|----------|------------------------|
| 8G       | 00_RUN_8G_7B           |
| 11G      | 00_RUN_11G_14B         |
| 12G      | 00_RUN_12G_14B         |
| 16G      | 00_RUN_16G_14B         |
| 24G      | 00_RUN_24G_14B         |

双击启动脚本，选择对应大小的模型，启动支持多实例的SakuraLLM。

请注意：8G 与 11G 脚本的设置对显存使用较为极限，建议在使用过程中关闭浏览器的硬件加速及占用显存的软件防爆显存。

## 3. 设置 AiNiee 参数
确保 AiNiee 已更新至最新版本（>= 4.71），然后启动 AiNiee，并根据显存大小设置以下选项：

| 选项 | 设置 |
|-----|-------|
| 翻译设置 - 进阶设置 - 使用 Tokens 模式      | 启用                                             |
| 翻译设置 - 进阶设置 - 每次翻译 Tokens       | 11G 设置为 512，8G/12G/16G/24G 设置为 640         |
| 翻译设置 - 进阶设置 - 最大线程数            | 8                                                |
| 翻译设置 - 进阶设置 - 错误重翻最大次数限制   | 1                                                |
| 翻译设置 - 进阶设置 - 保留换行符            | 启用                                              |

完成这些设置后，您已经完成了对性能的优化。

现在，在 4070 Ti Super 16G 级别的显卡上每小时可以处理 3M-5M 的 mtools 导出文本了，基本实现即翻即玩。

## 4. 原理说明
本节将对优化原理进行说明，帮助不满足于一键启动脚本的用户进行更细致的调整。

默认情况下，SakuraLLM 以单实例模式运行。

受限于模型端上下文长度的限制，翻译任务单次发送长度仅有数百字符，这种程度的任务单线程是无法跑满较新的GPU的。

通过启用多实例支持，一次性发送多个任务，可以一定程度上解决这个问题。

以下是`common.bat`中的启动参数示例：

```shell
.\llama\server.exe -m .\%model.name%.gguf -fa --no-mmap -cb -np %np% -c %ctx% -ngl %ngl% -a %model.name% --host 127.0.0.1
```

参数解释：

-cb：启动多实例支持；

-np：实例数量，应 <=8，在不爆显存且不爆上下文长度的前提下，值越大，性能越好；

-c：上下文长度，应该设置为 实例数量x单任务上下文长度，理论上单任务上下文长度越高越好，但是受限于模型能力与显存限制，实际无法设置的特别大，对于游戏导出文本来说，建议 1024 <= 单任务上下文长度 <= 2048；

## 5. 如何寻找能发挥出你的设备的极限性能的参数搭配
待续，有空继续写...
